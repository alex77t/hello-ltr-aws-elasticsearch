{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Elastic Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ltr.client import ElasticClient\n",
    "client = ElasticClient()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download & Build Index (run once)\n",
    "\n",
    "If you don't already have the downloaded dependencies; if you don't have TheMovieDB data indexed run this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ltr import download\n",
    "from ltr.index import rebuild\n",
    "from ltr.helpers.movies import indexable_movies\n",
    "\n",
    "corpus='http://es-learn-to-rank.labs.o19s.com/tmdb.json'\n",
    "judgments='http://es-learn-to-rank.labs.o19s.com/title_judgments.txt'\n",
    "download([corpus, judgments], dest='data/');\n",
    "\n",
    "movies=indexable_movies(movies='data/tmdb.json')\n",
    "rebuild(client, index='tmdb', doc_src=movies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features for movie titles\n",
    "\n",
    "We'll be searching movie titles (think searching for a specific movie on Netflix). And we have a set of judgments around the appropriatte movie to return. IE search for \"Star Wars\" return good star wars matches, in quality order...\n",
    "\n",
    "These cover various aspects of the problem (searching title by phrase, title bm25 score, release date, etc). We'll use this to explore and analyze a simple model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear TMDB's LTR\n",
    "client.reset_ltr(index='tmdb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.reset_ltr(index='tmdb')\n",
    "\n",
    "config = {\"validation\": {\n",
    "              \"index\": \"tmdb\",\n",
    "              \"params\": {\n",
    "                  \"keywords\": \"rambo\"\n",
    "              }\n",
    "    \n",
    "           },\n",
    "           \"featureset\": {\n",
    "            \"features\": [\n",
    "            { #1\n",
    "                \"name\": \"title_has_phrase\",\n",
    "                \"params\": [\"keywords\"],\n",
    "                \"template\": {\n",
    "                    \"constant_score\": {\n",
    "                        \"filter\": {\n",
    "                            \"match_phrase\": {\"title\": \"{{keywords}}\"}\n",
    "                        },\n",
    "                        \"boost\": 1.0\n",
    "                    }  \n",
    "                }\n",
    "            },\n",
    "            { #2\n",
    "                \"name\": \"title_has_terms\",\n",
    "                \"params\": [\"keywords\"],\n",
    "                \"template\": {\n",
    "                    \"constant_score\": {\n",
    "                        \"filter\": {\n",
    "                            \"match\": {\"title\": \"{{keywords}}\"}\n",
    "                        },\n",
    "                        \"boost\": 1.0\n",
    "                    }  \n",
    "                }\n",
    "            },\n",
    "            { #3\n",
    "                \"name\": \"title_bm25\",\n",
    "                \"params\": [\"keywords\"],\n",
    "                \"template\": {\n",
    "                    \"match\": {\"title\": \"{{keywords}}\"}\n",
    "                }\n",
    "            },\n",
    "            { #4\n",
    "                \"name\": \"overview_bm25\",\n",
    "                \"params\": [\"keywords\"],\n",
    "                \"template\": {\n",
    "                    \"match\": {\"overview\": \"{{keywords}}\"}\n",
    "                }\n",
    "            },\n",
    "            { #5\n",
    "                \"name\": \"overview_phrase_bm25\",\n",
    "                \"params\": [\"keywords\"],\n",
    "                \"template\": {\n",
    "                    \"match_phrase\": {\"overview\": \"{{keywords}}\"}\n",
    "                }\n",
    "            },\n",
    "            { #6\n",
    "                \"name\": \"title_fuzzy\",\n",
    "                \"params\": [\"keywords\"],\n",
    "                \"template\": {\n",
    "                    \"match\": {\"title\": \n",
    "                                {\"query\": \"{{keywords}}\",\n",
    "                                 \"fuzziness\": \"AUTO\"}}\n",
    "                }\n",
    "            },\n",
    "             { #7\n",
    "                \"name\": \"release_year\",\n",
    "                \"params\": [],\n",
    "                \"template\": {\n",
    "                    \"function_score\": {\n",
    "                        \"field_value_factor\": {\n",
    "                            \"field\": \"release_year\",\n",
    "                            \"missing\": 2000\n",
    "                        },\n",
    "                        \"query\": { \"match_all\": {} }\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            \n",
    "            ]\n",
    "    }}\n",
    "\n",
    "\n",
    "client.create_featureset(index='tmdb', name='title', ftr_config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Set Generation\n",
    "\n",
    "Log out features for each of the above queries out to a training set file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ltr.log import judgments_to_training_set\n",
    "trainingSet = judgments_to_training_set(client, \n",
    "                                        judgmentInFile='data/title_judgments.txt', \n",
    "                                        trainingOutFile='data/title_judgments_train.txt', \n",
    "                                        featureSet='title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stranger things <- 'bad query' -> 1\n",
    "#  Grades 0-4\n",
    "# 1\n",
    "# 2\n",
    "# 2\n",
    "# 0\n",
    "# 0\n",
    "# --> 2,2,1,0,0 <-- Perfect NDCG DCG / iDCG -> 1.0\n",
    "# iDCG is pretty bad\n",
    "# Ideal DCG 5\n",
    "#\n",
    "# rocky <- good query -> 1\n",
    "# 4\n",
    "# 4\n",
    "# 4\n",
    "# 3\n",
    "# Ideal DCG 25\n",
    "#\n",
    "#\n",
    "#\n",
    "# 4      <= iDCG\n",
    "# 4\n",
    "# 4\n",
    "# 4  \n",
    "\n",
    "# rambo\n",
    "# rocky\n",
    "# star wars\n",
    "# K fold cross validation\n",
    "# K=3 fold cross validation\n",
    "#\n",
    "# Fold 1\n",
    "# - train on two examples - rambo, rocky, target DCG@4\n",
    "# - test on the other one - star wars, how did we do DCG@4 = 10\n",
    "# Fold 2\n",
    "# - train on two examples - rambo, star wars, target DCG@4\n",
    "# - test on the other one - rocky, how did we do DCG@4 = 12\n",
    "# Fold 3\n",
    "# - train on two examples - rocky, star wars, target DCG@4\n",
    "# - test on the other one - rambo, how did we do DCG@4 = 14\n",
    "#\n",
    "# Training 20 <-- overfit!\n",
    "# AVG all the tests, 12\n",
    "#\n",
    "# Training 13 <-- not too bad! Our model can generalize!\n",
    "# AVG all the tests, 12\n",
    "#\n",
    "#\n",
    "# Full train on all the data -> model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# P@N <-- precision\n",
    "# NCDG@N <-- NDCG\n",
    "# DCG@N <-- DCG\n",
    "# ERR@N <-- ERR\n",
    "# MAP@N <-- Mean Avg Precision\n",
    "download(['http://es-learn-to-rank.labs.o19s.com/RankyMcRankFace.jar'], dest='data/');\n",
    "\n",
    "# LambdaMART / Gradient Boosting / MART\n",
    "#\n",
    "# (Ensemble for features 1..7)\n",
    "#  (Decision Trees) 6,3,2       \n",
    "#  (Decision Trees) 6,3,2\n",
    "#  (Decision Trees) 6,3,2\n",
    "#  (Decision Trees) 6,3,2\n",
    "#  (Decision Trees) 6,3,2\n",
    "#  (Decision Trees) 4,5,7\n",
    "#  (Decision Trees) 4,5,7\n",
    "#  (Decision Trees) 4,5,7\n",
    "#  (Decision Trees) 4,5,7\n",
    "#  (Decision Trees) 7,1,2\n",
    "\n",
    "#\n",
    "# Random Forests (bag=2, frate=0.6)\n",
    "#\n",
    "# 5->3\n",
    "# (Ensemble for 40% of features 1..7) 1,3,5,7,\n",
    "#  (Decision Trees) 5,3,1\n",
    "#  (Decision Trees) 5,3,1\n",
    "#  (Decision Trees) 5,3,1\n",
    "#  (Decision Trees) 5,3,1\n",
    "#  (Decision Trees) 5,7,1\n",
    "#  (Decision Trees) ...\n",
    "#  (Decision Trees)\n",
    "#  (Decision Trees)\n",
    "#  (Decision Trees)\n",
    "#  (Decision Trees)\n",
    "#\n",
    "# 6->2\n",
    "# (Ensemble for features 40% of features 1..7) 2,3,5,6\n",
    "#  (Decision Trees) 6,2,3\n",
    "#  (Decision Trees) 6,2,3\n",
    "#  (Decision Trees) 6,2,3\n",
    "#  (Decision Trees) 5,6,2\n",
    "#  (Decision Trees) ..\n",
    "#  (Decision Trees)\n",
    "#  (Decision Trees)\n",
    "#  (Decision Trees)\n",
    "#  (Decision Trees)\n",
    "#  (Decision Trees)\n",
    "\n",
    "# Feature 3 reduced error 12250.938430747574\n",
    "# Feature 6 reduced error 12167.279980776944\n",
    "# Feature 7 reduced error 11184.936996662022\n",
    "# Feature 4 reduced error 7342.442106757739\n",
    "# Feature 5 reduced error 5252.231534271401\n",
    "# Feature 1 reduced error 2586.7181503571337\n",
    "# Feature 2 reduced error 126.93687012857046\n",
    "\n",
    "#\n",
    "#-- FEATURE IMPACTS\n",
    "# Feature 7 reduced error 8.795923250386467E8\n",
    "# Feature 5 reduced error 7.232922151350935E8\n",
    "# Feature 1 reduced error 3.301986939497073E8\n",
    "# Feature 4 reduced error 3.1184694617029667E8\n",
    "# Feature 3 reduced error 2.5817983556409964E8\n",
    "# Feature 2 reduced error 0.0\n",
    "# Feature 6 reduced error 0.0\n",
    "\n",
    "\n",
    "!java -jar data/RankyMcRankFace.jar -ranker 6 -bag 10 -frate 0.6 -tree 10 \\\n",
    " -metric2t DCG@4 -train data/title_judgments_train.txt -save data/model.txt "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Search: which features work best?\n",
    "\n",
    "What combination of these features work best? Train a model with every combination, and use k-fold cross valudation (see `kcv=15` below). The combination with the best NDCG is output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ltr.train import feature_search\n",
    "rankLibResult, ndcgPerFeature = feature_search(client,\n",
    "                                               trainingInFile='data/title_judgments_train.txt',\n",
    "                                               metric2t='NDCG@10',\n",
    "                                               leafs=20,\n",
    "                                               trees=20,\n",
    "                                               kcv=15,\n",
    "                                               features=[1,2,3,4,5,6,7],\n",
    "                                               featureSet='title')\n",
    "\n",
    "print()\n",
    "print(\"Impact of each feature on the model\")\n",
    "trainLogs = rankLibResult.trainingLogs\n",
    "for ftrId, impact in trainLogs[-1].impacts.items():\n",
    "    print(\"{} - {}\".format(ftrId, impact))\n",
    "    \n",
    "for roundDcg in trainLogs[-1].rounds:\n",
    "    print(roundDcg)\n",
    "    \n",
    "print(\"Avg NDCG@10 when feature included:\")\n",
    "for ftrId, ndcg in ndcgPerFeature.items():\n",
    "    print(\"%s => %s\" % (ftrId, ndcg))\n",
    "    \n",
    "print(\"Avg K-Fold NDCG@10 %s\" % rankLibResult.kcvTestAvg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare to model w/ all features\n",
    "\n",
    "Compare the features output above (something like...)\n",
    "\n",
    "```\n",
    "Impact of each feature on the model\n",
    "3 - 17728.651362443747\n",
    "4 - 10986.88505569509\n",
    "7 - 7504.573843344015\n",
    "1 - 1580.321225819952\n",
    "```\n",
    "\n",
    "to one trained with the full model. Notice how features have different impacts. This is due to feature dependency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ltr import train\n",
    "trainLog  = train(client,\n",
    "                  trainingInFile='data/title_judgments_train.txt',\n",
    "                  metric2t='NDCG@10',\n",
    "                  leafs=20,\n",
    "                  trees=20,\n",
    "                  features=[3,4,7,1],\n",
    "                  index='tmdb',\n",
    "                  featureSet='title',\n",
    "                  modelName='title')\n",
    "\n",
    "print()\n",
    "print(\"Impact of each feature on the model\")\n",
    "for ftrId, impact in trainLog.impacts.items():\n",
    "    print(\"{} - {}\".format(ftrId, impact))\n",
    "    \n",
    "for roundDcg in trainLog.rounds:\n",
    "    print(roundDcg)\n",
    "    \n",
    "print(\"Train NDCG@10 %s\" % trainLog.rounds[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias towards fewer features\n",
    "\n",
    "By adding a 'cost', to feature search, we add a multiplier that punishes models with more features slightly. This results in a tiny bias towards simpler models all things being equal. As we'd prefer one that doesn't need to execute more features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ltr.train import feature_search\n",
    "rankLibResult, ndcgPerFeature = feature_search(client,\n",
    "                                               trainingInFile='data/title_judgments_train.txt',\n",
    "                                               metric2t='NDCG@10',\n",
    "                                               leafs=20,\n",
    "                                               trees=20,\n",
    "                                               kcv=15,\n",
    "                                               featureCost=0.02,# 1.0-cost ^ num_features\n",
    "                                               features=[1,2,3,4,5,6,7],\n",
    "                                               featureSet='title')\n",
    "\n",
    "print()\n",
    "print(\"Impact of each feature on the model\")\n",
    "trainLogs = rankLibResult.trainingLogs\n",
    "for ftrId, impact in trainLogs[-1].impacts.items():\n",
    "    print(\"{} - {}\".format(ftrId, impact))\n",
    "    \n",
    "for roundDcg in trainLogs[-1].rounds:\n",
    "    print(roundDcg)\n",
    "    \n",
    "print(\"Avg NDCG@10 when feature included:\")\n",
    "for ftrId, ndcg in ndcgPerFeature.items():\n",
    "    print(\"%s => %s\" % (ftrId, ndcg))\n",
    "    \n",
    "print(\"Avg K-Fold NDCG@10 %s\" % rankLibResult.kcvTestAvg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating the Model\n",
    "\n",
    "It's interesting to see what features our model makes use of, but we need guidance on adding additional features to the model. We know our model is an ensemble of decision trees. Wouldn't it be cool if we could trace where documents end up on that decision tree?\n",
    "\n",
    "Specifically, we care about problems. Or what we will call affectionately *whoopsies*. \n",
    "\n",
    "As a 'whoopsie' example, consider the query \"Rambo\". if a '0' document like 'First Daughter' ranked the same or higher than a '4' document (\"Rambo\")., that's a problem. It's also an opportunity for improvement. We'd want to isolate that, see if it's indicative of a broader trend, and thus worth adding a feature for.\n",
    "\n",
    "Let's see a concrete example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ltr.client import ElasticClient\n",
    "from ltr.MART_model import eval_model\n",
    "from ltr.judgments import judgments_from_file, judgments_by_qid\n",
    "\n",
    "features, _ = client.feature_set(index='tmdb', name='title')\n",
    "\n",
    "with open('data/title_judgments_train.txt') as f:\n",
    "    judgmentDict = judgments_by_qid(judgments_from_file(f))\n",
    "\n",
    "\n",
    "rambo=judgmentDict[1]\n",
    "model = eval_model(modelName='title',\n",
    "                   features=features,\n",
    "                   judgments=rambo)\n",
    "\n",
    "print()\n",
    "print(\"## Evaluating graded docs for search keywords '%s'\" % rambo[0].keywords)\n",
    "print()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examining our evaluation for whoopsies\n",
    "\n",
    "Let's looks at one tree in our ensemble, te see how it was evaluated.\n",
    "\n",
    "```\n",
    "if title_bm25 > 10.664251:\n",
    "  if title_phrase > 0.0:\n",
    "    if title_bm25 > 13.815164:\n",
    "      if release_year > 2000.0:\n",
    "        <= 0.1215(0/0/)\n",
    "      else:\n",
    "        <= 0.1240(0/0/)\n",
    "    else:\n",
    "      if title_bm25 > 10.667803:\n",
    "        if overview_bm25 > 0.0:\n",
    "          <= 0.1194(0/0/)\n",
    "        else:\n",
    "          <= 0.1161(1/0/)\n",
    "      else:\n",
    "        <= 0.1264(0/0/)\n",
    "  else:\n",
    "    <= 0.0800(0/0/)\n",
    "else:\n",
    "  if title_phrase > 0.0:\n",
    "    if title_bm25 > 8.115499:\n",
    "      if title_bm25 > 8.217656:\n",
    "        <= 0.1097(2/1/qid:40:2(12180)-3(140607))\n",
    "      else:\n",
    "        <= 0.1559(0/0/)\n",
    "    else:\n",
    "      <= -0.0021(2/1/qid:40:2(1895)-3(330459))\n",
    "  else:\n",
    "    <= -0.1093(25/1/qid:40:0(85783)-3(1892))\n",
    "```\n",
    "\n",
    "You'll notice here this tree is represented by a series of if statements, where the feature's name is used. This is handy as it lets us take apart the structure of the tree.\n",
    "\n",
    "You'll also notice the leaf nodes starting with \n",
    "\n",
    "```\n",
    "<=\n",
    "```\n",
    "\n",
    "These leaf nodes have a floating point value, corresponding to the relevance score that documents ending up here will have. Each leaf also has three items in paranthesis, such as `(2/1/qid:40:2(1895)-3(330459))`. This is a report summarizing the result of evaluating the tree on the provided judgment list. Indicating:\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "   +--- 2 Documents evaluated to this leaf node                   +-- max grade doc eval'd to this leaf\n",
    "   |                                                              |\n",
    "   | +----- 1 'whoopsie' occured                                  |  +-- corresp. doc id of max doc\n",
    "   | |                                                            |  |\n",
    "   | |   +--- details on each whoopsie ----------- qid:40:2(1985)-3(330459)\n",
    "   | |   |                                              | |  |\n",
    "  (2/1/qid:40:2(1895)-3(330459))                        | |  |\n",
    "                                                        | |  + doc id of min graded doc\n",
    "                                                        | |\n",
    "                                                        | + min grade of docs eval'd to this leaf\n",
    "                                                        |\n",
    "                                                        + query id of whoopsie from judgments\n",
    "```\n",
    "\n",
    "\n",
    "Looking at Star Wars, our biggest issues in this tree are with the bottom-most leaf. Here\n",
    "\n",
    "```\n",
    "if title_bm25 > 10.664251:\n",
    "  ...\n",
    "else:\n",
    "  if title_phrase > 0.0:\n",
    "    ...\n",
    "  else:\n",
    "    <= -0.1093(25/1/qid:40:0(85783)-3(1892))\n",
    "```\n",
    "\n",
    "\n",
    "Document 85783 (a '0') and doc 1892 are given the same grade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Doc in index... (notice nothing mentions 'star wars')\n",
    "\n",
    "client.get_doc(index='tmdb', doc_id=1368)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.get_doc(index='tmdb', doc_id=1368)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add a feature: collection name\n",
    "\n",
    "We have an intuition about our data, there is a field for the movies \"collection name\". See it here below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ltr.helpers.movies import get_movie\n",
    "get_movie(1368)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll add collection name, and reindex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ltr import download\n",
    "from ltr.index import rebuild\n",
    "from ltr.helpers.movies import indexable_movies\n",
    "\n",
    "from ltr.client import ElasticClient\n",
    "client = ElasticClient()\n",
    "\n",
    "def add_char_names(src_movie, base_doc):\n",
    "    if 'belongs_to_collection' in src_movie and src_movie['belongs_to_collection'] is not None:\n",
    "        if 'name' in src_movie['belongs_to_collection']:\n",
    "            base_doc['collection_name'] = src_movie['belongs_to_collection']['name']\n",
    "    if 'cast' in src_movie and src_movie['cast'] is not None:\n",
    "        characters = []\n",
    "        for cast_entry in src_movie['cast']:\n",
    "            characters.append(cast_entry['character'])\n",
    "        base_doc['secondary_characters'] = characters[10:]\n",
    "        base_doc['prom_characters'] = characters[:10]\n",
    "    return base_doc\n",
    "\n",
    "\n",
    "movies=indexable_movies(movies='data/tmdb.json', enrich=add_char_names)\n",
    "rebuild(client, index='tmdb', doc_src=movies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confirm it's in our doc now..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.get_doc(index='tmdb', doc_id=1368)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add it to the features, and retrain...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"validation\": {\n",
    "              \"index\": \"tmdb\",\n",
    "              \"params\": {\n",
    "                  \"keywords\": \"rambo\"\n",
    "              }\n",
    "    \n",
    "           },\n",
    "           \"featureset\": {\n",
    "            \"features\": [ \n",
    "            {  #1\n",
    "                \"name\": \"title_phrase\",\n",
    "                \"params\": [\"keywords\"],\n",
    "                \"template\": {\n",
    "                    \"constant_score\": {\n",
    "                        \"filter\": {\n",
    "                            \"match_phrase\": {\"title\": \"{{keywords}}\"}\n",
    "                        },\n",
    "                        \"boost\": 1.0\n",
    "                    }  \n",
    "                }\n",
    "            },\n",
    "            { #2\n",
    "                \"name\": \"title\",\n",
    "                \"params\": [\"keywords\"],\n",
    "                \"template\": {\n",
    "                    \"constant_score\": {\n",
    "                        \"filter\": {\n",
    "                            \"match\": {\"title\": \"{{keywords}}\"}\n",
    "                        },\n",
    "                        \"boost\": 1.0\n",
    "                    }  \n",
    "                }\n",
    "            },\n",
    "            { #3\n",
    "                \"name\": \"title_bm25\",\n",
    "                \"params\": [\"keywords\"],\n",
    "                \"template\": {\n",
    "                    \"match\": {\"title\": \"{{keywords}}\"}\n",
    "                }\n",
    "            },\n",
    "            { #4\n",
    "                \"name\": \"overview_bm25\",\n",
    "                \"params\": [\"keywords\"],\n",
    "                \"template\": {\n",
    "                    \"match\": {\"overview\": \"{{keywords}}\"}\n",
    "                }\n",
    "            },\n",
    "            { #5\n",
    "                \"name\": \"overview_phrase_bm25\",\n",
    "                \"params\": [\"keywords\"],\n",
    "                \"template\": {\n",
    "                    \"match_phrase\": {\"overview\": \"{{keywords}}\"}\n",
    "                }\n",
    "            },\n",
    "            { #6\n",
    "                \"name\": \"title_fuzzy\",\n",
    "                \"params\": [\"keywords\"],\n",
    "                \"template\": {\n",
    "                    \"match\": {\"title\": \n",
    "                                {\"query\": \"{{keywords}}\",\n",
    "                                 \"fuzziness\": \"AUTO\"}}\n",
    "                }\n",
    "            },\n",
    "             { #7\n",
    "                \"name\": \"release_year\",\n",
    "                \"params\": [],\n",
    "                \"template\": {\n",
    "                    \"function_score\": {\n",
    "                        \"field_value_factor\": {\n",
    "                            \"field\": \"release_year\",\n",
    "                            \"missing\": 2000\n",
    "                        },\n",
    "                        \"query\": { \"match_all\": {} }\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            { #8\n",
    "                \"name\": \"secondary_char_name_bm25\",\n",
    "                \"params\": [\"keywords\"],\n",
    "                \"template\": {\n",
    "                    \"match\": {\"secondary_characters\": \n",
    "                                {\"query\": \"{{keywords}}\"}}\n",
    "                }\n",
    "            },\n",
    "            { #9\n",
    "                \"name\": \"prom_char_bm25\",\n",
    "                \"params\": [\"keywords\"],\n",
    "                \"template\": {\n",
    "                    \"match\": {\"prom_characters\": \n",
    "                                {\"query\": \"{{keywords}}\"}}\n",
    "                }\n",
    "            },\n",
    "            { # 10\n",
    "                \"name\": \"coll_name_bm25\",\n",
    "                \"params\": [\"keywords\"],\n",
    "                \"template\": {\n",
    "                    \"match\": {\"collection_name\": \n",
    "                                {\"query\": \"{{keywords}}\"}}\n",
    "                }\n",
    "            },\n",
    "            { # 11\n",
    "                \"name\": \"coll_name_phrase_bm25\",\n",
    "                \"params\": [\"keywords\"],\n",
    "                \"template\": {\n",
    "                    \"match_phrase\": {\"collection_name\": \n",
    "                                {\"query\": \"{{keywords}}\"}}\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            \n",
    "            ] #qid:1 red high heels -> keywords, query_understanding_color, item_type\n",
    "    }}\n",
    "\n",
    "client.create_featureset(index='tmdb', name='title2', ftr_config=config)\n",
    "\n",
    "from ltr.log import judgments_to_training_set\n",
    "trainingSet = judgments_to_training_set(client, \n",
    "                                        judgmentInFile='data/title_judgments.txt', \n",
    "                                        trainingOutFile='data/title2_judgments_train.txt', \n",
    "                                        featureSet='title2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ltr.train import kcv\n",
    "res  = kcv(client,\n",
    "            trainingInFile='data/title2_judgments_train.txt',\n",
    "            metric2t='NDCG@10',\n",
    "            leafs=20,\n",
    "            trees=20,\n",
    "            ranker=8, # Use a \"Random Forests Model\"\n",
    "            frate=0.5,\n",
    "            bag=10, # Number of ensembles in the forest bag=1, 1 LambdaMART model with random features chosen\n",
    "            index='tmdb',\n",
    "            kcv=5,\n",
    "            features=[1,2,3,4,5,6,7,8,9,10,11],\n",
    "            featureSet='title2',\n",
    "            modelName='title2')\n",
    "\n",
    "print()\n",
    "print(\"Impact of each feature on the model\")\n",
    "for ftrId, impact in res.trainingLogs[0].impacts.items():\n",
    "    print(\"{} - {}\".format(ftrId, impact))\n",
    "    \n",
    "    \n",
    "print(\"Test NDCG@10 %s\" % res.kcvTestAvg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ltr.train import feature_search\n",
    "rankLibResult, ndcgPerFeature = feature_search(client,\n",
    "                                               trainingInFile='data/title2_judgments_train.txt',\n",
    "                                               metric2t='NDCG@10',\n",
    "                                               leafs=20,\n",
    "                                               trees=20,\n",
    "                                               kcv=15,\n",
    "                                               features=[1,2,3,4,5,6,7,8,9],\n",
    "                                               featureSet='title2')\n",
    "\n",
    "print()\n",
    "print(\"Impact of each feature on the model\")\n",
    "trainLogs = rankLibResult.trainingLogs\n",
    "for ftrId, impact in trainLogs[-1].impacts.items():\n",
    "    print(\"{} - {}\".format(ftrId, impact))\n",
    "    \n",
    "for roundDcg in trainLogs[-1].rounds:\n",
    "    print(roundDcg)\n",
    "    \n",
    "print(\"Avg NDCG@10 when feature included:\")\n",
    "for ftrId, ndcg in ndcgPerFeature.items():\n",
    "    print(\"%s => %s\" % (ftrId, ndcg))\n",
    "    \n",
    "print(\"Avg K-Fold NDCG@10 %s\" % rankLibResult.kcvTestAvg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ltr import train\n",
    "trainLog  = train(client,\n",
    "                  trainingInFile='data/title2_judgments_train.txt',\n",
    "                  metric2t='NDCG@10',\n",
    "                  leafs=20,\n",
    "                  trees=20,\n",
    "                  index='tmdb',\n",
    "                  features=[1,4,6,7,8],\n",
    "                  featureSet='title2',\n",
    "                  modelName='title2')\n",
    "\n",
    "print()\n",
    "print(\"Impact of each feature on the model\")\n",
    "for ftrId, impact in trainLog.impacts.items():\n",
    "    print(\"{} - {}\".format(ftrId, impact))\n",
    "    \n",
    "for roundDcg in trainLog.rounds:\n",
    "    print(roundDcg)\n",
    "    \n",
    "print(\"Train NDCG@10 %s\" % trainLog.rounds[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ltr.train import kcv\n",
    "res  = kcv(client,\n",
    "            trainingInFile='data/title2_judgments_train.txt',\n",
    "            metric2t='NDCG@10',\n",
    "            leafs=4,\n",
    "            trees=100,\n",
    "            ranker=8, # Use a \"Random Forests Model\"\n",
    "            frate=0.5,\n",
    "            bag=100, # Number of ensembles in the forest bag=1, 1 LambdaMART model with random features chosen\n",
    "            index='tmdb',\n",
    "            kcv=5,\n",
    "            features=[1,2,3,4,5,6,7,8,9],\n",
    "            featureSet='title2',\n",
    "            modelName='title_rf')\n",
    "\n",
    "print()\n",
    "print(\"Impact of each feature on the model\")\n",
    "for ftrId, impact in res.trainingLogs[0].impacts.items():\n",
    "    print(\"{} - {}\".format(ftrId, impact))\n",
    "    \n",
    "    \n",
    "print(\"Test NDCG@10 %s\" % res.kcvTestAvg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ltr import search\n",
    "search(client, \"basketball cartoon aliens\", modelName='title2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.reset_ltr(index='tmdb')\n",
    "\n",
    "config = {\"validation\": {\n",
    "              \"index\": \"tmdb\",\n",
    "              \"params\": {\n",
    "                  \"keywords\": \"rambo\"\n",
    "              }\n",
    "    \n",
    "           },\n",
    "           \"featureset\": {\n",
    "            \"features\": [\n",
    "            {\n",
    "                \"name\": \"title_phrase\",\n",
    "                \"params\": [\"keywords\"],\n",
    "                \"template\": {\n",
    "                    \"constant_score\": {\n",
    "                        \"filter\": {\n",
    "                            \"match_phrase\": {\"title\": \"{{keywords}}\"}\n",
    "                        },\n",
    "                        \"boost\": 1.0\n",
    "                    }  \n",
    "                }\n",
    "            },\n",
    " \n",
    "            {\n",
    "                \"name\": \"overview_bm25\",\n",
    "                \"params\": [\"keywords\"],\n",
    "                \"template\": {\n",
    "                    \"match\": {\"overview\": \"{{keywords}}\"}\n",
    "                }\n",
    "            },\n",
    "   \n",
    "            {\n",
    "                \"name\": \"title_fuzzy\",\n",
    "                \"params\": [\"keywords\"],\n",
    "                \"template\": {\n",
    "                    \"match\": {\"title\": \n",
    "                                {\"query\": \"{{keywords}}\",\n",
    "                                 \"fuzziness\": \"AUTO\"}}\n",
    "                }\n",
    "            },\n",
    "             {\n",
    "                \"name\": \"release_year\",\n",
    "                \"params\": [],\n",
    "                \"template\": {\n",
    "                    \"function_score\": {\n",
    "                        \"field_value_factor\": {\n",
    "                            \"field\": \"release_year\",\n",
    "                            \"missing\": 2000\n",
    "                        },\n",
    "                        \"query\": { \"match_all\": {} }\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"coll_name_bm25\",\n",
    "                \"params\": [\"keywords\"],\n",
    "                \"template\": {\n",
    "                    \"match\": {\"collection_name\": \n",
    "                                {\"query\": \"{{keywords}}\"}}\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"title_fuzzy_2\",\n",
    "                \"params\": [\"keywords\"],\n",
    "                \"template\": {\n",
    "                       \"match\": {\"title.as_str\": \n",
    "                                {\"query\": \"{{keywords}}\",\n",
    "                                 \"fuzziness\": \"AUTO\"}}\n",
    "                }\n",
    "            }\n",
    "            ]\n",
    "    }}\n",
    "\n",
    "\n",
    "client.create_featureset(index='tmdb', name='title2', ftr_config=config)\n",
    "\n",
    "from ltr.log import judgments_to_training_set\n",
    "trainingSet = judgments_to_training_set(client, \n",
    "                                        judgmentInFile='data/title_judgments.txt', \n",
    "                                        trainingOutFile='data/title2_judgments_train.txt', \n",
    "                                        featureSet='title2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.get_doc(index='tmdb', doc_id=1892)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ltr.train import feature_search\n",
    "rankLibResult, ndcgPerFeature = feature_search(client,\n",
    "                                               trainingInFile='data/title2_judgments_train.txt',\n",
    "                                               metric2t='NDCG@10',\n",
    "                                               leafs=20,\n",
    "                                               trees=20,\n",
    "                                               kcv=15,\n",
    "                                               features=[1,2,3,4,5],\n",
    "                                               featureSet='title2')\n",
    "\n",
    "print()\n",
    "print(\"Impact of each feature on the model\")\n",
    "trainLogs = rankLibResult.trainingLogs\n",
    "for ftrId, impact in trainLogs[-1].impacts.items():\n",
    "    print(\"{} - {}\".format(ftrId, impact))\n",
    "    \n",
    "for roundDcg in trainLogs[-1].rounds:\n",
    "    print(roundDcg)\n",
    "    \n",
    "print(\"Avg NDCG@10 when feature included:\")\n",
    "for ftrId, ndcg in ndcgPerFeature.items():\n",
    "    print(\"%s => %s\" % (ftrId, ndcg))\n",
    "    \n",
    "print(\"Avg K-Fold NDCG@10 %s\" % rankLibResult.kcvTestAvg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ltr import train\n",
    "trainLog  = train(client,\n",
    "                  trainingInFile='data/title2_judgments_train.txt',\n",
    "                  metric2t='NDCG@10',\n",
    "                  leafs=20,\n",
    "                  trees=20,\n",
    "                  index='tmdb',\n",
    "                  featureSet='title2',\n",
    "                  modelName='title2')\n",
    "\n",
    "print()\n",
    "print(\"Impact of each feature on the model\")\n",
    "for ftrId, impact in trainLog.impacts.items():\n",
    "    print(\"{} - {}\".format(ftrId, impact))\n",
    "    \n",
    "for roundDcg in trainLog.rounds:\n",
    "    print(roundDcg)\n",
    "    \n",
    "print(\"Train NDCG@10 %s\" % trainLog.rounds[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ltr.client import ElasticClient\n",
    "from ltr.MART_model import eval_model\n",
    "from ltr.judgments import judgments_from_file, judgments_by_qid\n",
    "\n",
    "features, _ = client.feature_set(index='tmdb', name='title2')\n",
    "\n",
    "with open('data/title_judgments_train.txt') as f:\n",
    "    judgmentDict = judgments_by_qid(judgments_from_file(f))\n",
    "\n",
    "for qid, judgments in judgmentDict.items():\n",
    "\n",
    "    model = eval_model(modelName='title2',\n",
    "                           features=features,\n",
    "                           judgments=judgments)\n",
    "\n",
    "    print()\n",
    "    print(\"## Evaluating graded docs for search keywords '%s'\" % judgments[0].keywords)\n",
    "    print()\n",
    "    print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.get_doc(index='tmdb', doc_id=860)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
